# üìù Publications 
<!-- Âä†ÁÇπË°®ÊÉÖÂåÖ,Áõ¥Êé•Â§çÂà∂ÂõæÁâáÂç≥ÂèØ  https://github.com/guodongxiaren/README/blob/master/emoji.md?tdsourcetag=s_pcqq_aiomsg -->



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2024</div><img src='images/mm2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**ACM MM 2024**] [Video Bokeh Rendering: Make Casual Videography Cinematic](https://dlnext.acm.org/doi/10.1145/3664647.3680629) \\
Yawen Luo, Min Shi, **Liao Shen**, Yachuan Huang, Zixuan Ye, Juewen Peng, Zhiguo Cao. \\
[[Paper]](https://dlnext.acm.org/doi/10.1145/3664647.3680629)

We introduce VBR, the video bokeh rendering model that first leverages information from multiple frames to generate refocusable videos from all-in-focus videos.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/dreammover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**ECCV 2024**] [DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion](https://dreamm0ver.github.io/) \\
**Liao Shen**, Tianqi Liu, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, Zhiguo Cao. \\
[[Project page]](https://dreamm0ver.github.io/)
[[Paper]](https://arxiv.org/abs/2409.09605)
[[Code]](https://github.com/leoShen917/DreamMover)

By leveraging the prior of diffusion models, DreamMover can generate intermediate images from image pairs with large motion while maintaining semantic consistency.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/MVSGaussian.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ECCV 2024**] [MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo](https://mvsgaussian.github.io/) \\
Tianqi Liu, Guangcong Wang, Shoukang Hu, **Liao Shen**, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.  \\
[[Project page]](https://mvsgaussian.github.io/)
[[Paper]](https://arxiv.org/abs/2405.12218)
[[Code]](https://github.com/TQTQliu/MVSGaussian)

MVSGaussian is a Gaussian-based method designed for efficient reconstruction of unseen scenes from sparse views in a single forward pass. It offers high-quality initialization for fast training and real-time rendering.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV Workshop 2024</div><img src='images/eccvw2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ECCV Workshop 2024**] [MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo](https://mvsgaussian.github.io/) \\
Junrui Zhang, Jiaqi Li, Yachuan Huang, Yiran Wang, Jinghong Zheng, **Liao Shen**, Zhiguo Cao  \\
[[Project page]](https://mvsgaussian.github.io/)
[[Paper]](https://arxiv.org/abs/2405.12218)
[[Code]](https://github.com/TQTQliu/MVSGaussian)

MVSGaussian is a Gaussian-based method designed for efficient reconstruction of unseen scenes from sparse views in a single forward pass. It offers high-quality initialization for fast training and real-time rendering.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/cvpr2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2024**]  [DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video](https://arxiv.org/abs/2403.10103) \\
Huiqiang Sun, Xingyi Li, **Liao Shen**, Xinyi Ye, Ke Xian, Zhiguo Cao.  \\
[[Project page]](https://huiqiang-sun.github.io/dyblurf/)
[[Paper]](https://arxiv.org/abs/2403.10103)
[[Code]](https://github.com/huiqiang-sun/DyBluRF)

DyBluRF is a dynamic neural radiance field method that synthesizes sharp novel views from a monocular video affected by motion blur.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2023</div><img src='images/acmmm2023.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ACM MM 2023**] [Make-It-4D: Synthesizing a Consistent Long-Term Dynamic Scene Video from a Single Image](https://arxiv.org/abs/2308.10257) \\
**Liao Shen**, Xingyi Li, Huiqiang Sun, Juewen Peng, Ke Xian, Zhiguo Cao, Guosheng Lin. \\
[[Paper]](https://arxiv.org/abs/2308.10257)
[[Code]](https://github.com/leoShen917/Make-It-4D)

Make-It-4D is a novel framework that can generate a consistent long-term dynamic video from a single image. The generated video in volves both visual content movements and large camera motions, bringing the still image back to life.

</div>
</div>
