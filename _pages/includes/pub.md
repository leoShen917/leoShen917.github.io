# üìù Publications 
<!-- Âä†ÁÇπË°®ÊÉÖÂåÖ,Áõ¥Êé•Â§çÂà∂ÂõæÁâáÂç≥ÂèØ  https://github.com/guodongxiaren/README/blob/master/emoji.md?tdsourcetag=s_pcqq_aiomsg -->



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2024</div><img src='images/mm2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ACM MM 2024**] [Video Bokeh Rendering: Make Casual Videography Cinematic](https://dlnext.acm.org/doi/10.1145/3664647.3680629) **<font color=red>(Best paper candidate)</font>**üöÄüöÄüöÄ \\
Yawen Luo, Min Shi, **Liao Shen**, Yachuan Huang, Zixuan Ye, Juewen Peng, Zhiguo Cao.\\
[[Paper]](https://dlnext.acm.org/doi/10.1145/3664647.3680629)

We introduce VBR, the video bokeh rendering model that first leverages information from multiple frames to generate refocusable videos from all-in-focus videos.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/dreammover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**ECCV 2024**] [DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion](https://dreamm0ver.github.io/) \\
**Liao Shen**, Tianqi Liu, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, Zhiguo Cao. \\
[[Project page]](https://dreamm0ver.github.io/)
[[Paper]](https://arxiv.org/abs/2409.09605)
[[Code]](https://github.com/leoShen917/DreamMover)

By leveraging the prior of diffusion models, DreamMover can generate intermediate images from image pairs with large motion while maintaining semantic consistency.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/MVSGaussian.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ECCV 2024**] [MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo](https://mvsgaussian.github.io/) \\
Tianqi Liu, Guangcong Wang, Shoukang Hu, **Liao Shen**, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.  \\
[[Project page]](https://mvsgaussian.github.io/)
[[Paper]](https://arxiv.org/abs/2405.12218)
[[Code]](https://github.com/TQTQliu/MVSGaussian)

MVSGaussian is a Gaussian-based method designed for efficient reconstruction of unseen scenes from sparse views in a single forward pass. It offers high-quality initialization for fast training and real-time rendering.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV Workshop 2024</div><img src='images/eccvw2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ECCV Workshop 2024**] [Towards Robust Monocular Depth Estimation in Non-Lambertian Surfaces](https://arxiv.org/pdf/2408.06083) \\
Junrui Zhang, Jiaqi Li, Yachuan Huang, Yiran Wang, Jinghong Zheng, **Liao Shen**, Zhiguo Cao  \\
[[Paper]](https://arxiv.org/pdf/2408.06083)

Monocular depth estimation methods often fail in predicting non-Lambertian surfaces, such as transparent or mirror (ToM) surfaces, due to the unique reflective properties of these regions. In this work, We enable the baseline model to directly learn the uniqueness of non-Lambertian surface regions for depth estimation through a well designed training framework.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/cvpr2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2024**]  [DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video](https://arxiv.org/abs/2403.10103) \\
Huiqiang Sun, Xingyi Li, **Liao Shen**, Xinyi Ye, Ke Xian, Zhiguo Cao.  \\
[[Project page]](https://huiqiang-sun.github.io/dyblurf/)
[[Paper]](https://arxiv.org/abs/2403.10103)
[[Code]](https://github.com/huiqiang-sun/DyBluRF)

DyBluRF is a dynamic neural radiance field method that synthesizes sharp novel views from a monocular video affected by motion blur.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2023</div><img src='images/acmmm2023.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**ACM MM 2023**] [Make-It-4D: Synthesizing a Consistent Long-Term Dynamic Scene Video from a Single Image](https://arxiv.org/abs/2308.10257) \\
**Liao Shen**, Xingyi Li, Huiqiang Sun, Juewen Peng, Ke Xian, Zhiguo Cao, Guosheng Lin. \\
[[Paper]](https://arxiv.org/abs/2308.10257)
[[Code]](https://github.com/leoShen917/Make-It-4D)

Make-It-4D is a novel framework that can generate a consistent long-term dynamic video from a single image. The generated video in volves both visual content movements and large camera motions, bringing the still image back to life.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR Workshop 2023</div><img src='images/cvprw2023.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR Workshop 2023**] [Selective Bokeh Effect Transformation](https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Peng_Selective_Bokeh_Effect_Transformation_CVPRW_2023_paper.pdf) \\
Juewen Peng, Zhiyu Pan, Chengxin Liu, Xianrui Luo, Huiqiang Sun, **Liao Shen**, Ke Xian, Zhiguo Cao. \\
[[Paper]](https://github.com/JuewenPeng/SBTNet/blob/main/pdf/Selective Bokeh Effect Transformation.pdf)
[[Code]](https://github.com/JuewenPeng/SBTNet/tree/main)

Bokeh effect transformation aims to convert bokeh effects from one camera lens to another. In this work, we introduce a new concept of blur ratio, which represents the ratio of  the blur amount of a target image to that of a source image, and propose a novel framework SBTNet

</div>
</div>
