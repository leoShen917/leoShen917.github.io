# üìù Publications 
<!-- Âä†ÁÇπË°®ÊÉÖÂåÖ,Áõ¥Êé•Â§çÂà∂ÂõæÁâáÂç≥ÂèØ  https://github.com/guodongxiaren/README/blob/master/emoji.md?tdsourcetag=s_pcqq_aiomsg -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/MVSGaussian.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ECCV 2024**] [MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo](https://mvsgaussian.github.io/) \\
**Tianqi Liu**, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.  \\
[[Project page]](https://mvsgaussian.github.io/)
[[Paper]](https://arxiv.org/abs/2405.12218)
[[Code]](https://github.com/TQTQliu/MVSGaussian)
[[Video]](https://youtu.be/4TxMQ9RnHMA)
[[‰∏≠ÊñáËß£ËØª]](https://mp.weixin.qq.com/s/Y9uXxNMgliV9p-ne_bGpEw)

MVSGaussian is a Gaussian-based method designed for efficient reconstruction of unseen scenes from sparse views in a single forward pass. It offers high-quality initialization for fast training and real-time rendering.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/dreammover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[**ECCV 2024**] [DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion](https://dreamm0ver.github.io/) \\
Liao Shen, **Tianqi Liu**, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, Zhiguo Cao. \\
[[Project page]](https://dreamm0ver.github.io/)
[[Paper]](https://arxiv.org/abs/2409.09605)
[[Code]](https://github.com/leoShen917/DreamMover)

By leveraging the prior of diffusion models, DreamMover can generate intermediate images from image pairs with large motion while maintaining semantic consistency.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/wildavatar.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**arXiv 2024**] [WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation](https://wildavatar.github.io/) \\
Zihao Huang, Shoukang Hu, Guangcong Wang, **Tianqi Liu**, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu. \\
[[Project page]](https://wildavatar.github.io/)
[[Paper]](https://arxiv.org/pdf/2407.02165)
[[Code]](https://github.com/wildavatar/WildAvatar_Toolbox)
[[Video]](https://youtu.be/nbK2n2rFJ0E)

We present WildAvatar, a web-scale in-the-wild video dataset for 3D avatar creation.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/gefu.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2024**] [Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields](https://gefucvpr24.github.io/) \\
**Tianqi Liu**, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao. \\
[[Project page]](https://gefucvpr24.github.io/)
[[Paper]](https://arxiv.org/abs/2404.17528)
[[Code]](https://github.com/TQTQliu/GeFu)
[[Video]](https://youtu.be/Z4RgnsKF3Gs)


We present GeFu, a generalizable NeRF method that synthesizes novel views from multi-view images in a single forward pass.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/RStab.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2024**] [3D Multi-frame Fusion for Video Stabilization](https://arxiv.org/pdf/2404.12887) \\
Zhan Peng, Xinyi Ye, Weiyue Zhao, **Tianqi Liu**, Huiqiang Sun, Baopu Li, Zhiguo Cao. \\
[[Paper]](https://arxiv.org/pdf/2404.12887)
[[Code]](https://github.com/pzzz-cv/RStab)

RStab is a novel framework for video stabilization that integrates 3D multi-frame fusion through volume rendering.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/ETMVSNet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICCV 2023**] [When Epipolar Constraint Meets Non-local Operators in Multi-View Stereo](https://arxiv.org/abs/2309.17218) \\
**Tianqi Liu**, Xinyi Ye, Weiyue Zhao, Zhiyu Pan, Min Shi, Zhiguo Cao. \\
[[Paper]](https://arxiv.org/abs/2309.17218)
[[Code]](https://github.com/TQTQliu/ET-MVSNet)

ETMVSNet uses epipolar geometric priors to constrain feature aggregation fileds, thereby efficiently inferring multi-view depths and reconstructing scenes.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/dmvsnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICCV 2023**] [Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells](https://arxiv.org/abs/2307.09160) \\
Xinyi Ye, Weiyue Zhao, **Tianqi Liu**, Zihao Huang, Zhiguo Cao, Xin Li. \\
[[Paper]](https://arxiv.org/abs/2307.09160)
[[Code]](https://github.com/DIVE128/DMVSNet)

DMVSNet proposes a new perspective to consider the depth geometry of multi-view stereo and introduces a dual-depth approach to approximate the depth geometry with saddle-shaped cells.
</div>
</div>
