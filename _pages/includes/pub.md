# üìù Publications 
<!-- Âä†ÁÇπË°®ÊÉÖÂåÖ,Áõ¥Êé•Â§çÂà∂ÂõæÁâáÂç≥ÂèØ  https://github.com/guodongxiaren/README/blob/master/emoji.md?tdsourcetag=s_pcqq_aiomsg -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/cinectrl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Arxiv**] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://www.arxiv.org/pdf/2511.12921) \\
Huiqiang Sun\*, **Liao Shen\***, Zhan Peng, Kun Wang, Size Wu, Yuhang Zang, Tianqi Liu, Zihao Huang, Xingyu Zeng, Zhiguo Cao, Wei Li, Chen Change Loy \\
[[Paper]](https://www.arxiv.org/pdf/2511.12921)

Cinectrl is the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed).

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/ipro.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[**Arxiv**] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://ipro-alimama.github.io/) \\
**Liao Shen\***, Wentao Jiang\*, Yizhu Ran, Jiahe Li, Tiezheng Ge, Zhiguo Cao, Bo Zheng. \\
[[Project page]](https://ipro-alimama.github.io/)
[[Paper]](https://arxiv.org/abs/2510.14255)
[[Code]](http://alibaba.github.io/ROLL/docs/UserGuide/algorithms/Reward_FL)

IPRO is the first identity-preserving image-to-video generation model via reward feedback learning.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/mugs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICCV 2025**] [MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction](https://arxiv.org/abs/2508.04297) \\
Yaopeng Lou, **Liao Shen**, Tianqi Liu, Jiaqi Li, Zihao Huang, Huiqiang Sun, Zhiguo Cao. \\
[[Paper]](https://arxiv.org/abs/2508.04297)

MuGS is the first multi-baseline generalizable gaussian splatting method.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/free4d.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICCV 2025**] [Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency](https://free4d.github.io/) \\
Tianqi Liu, Zihao Huang, Zhaoxi Chen, Guangcong Wang, Shoukang Hu, **Liao Shen**, Huiqiang Sun, Zhiguo Cao, Wei Li, Ziwei Liu. \\
[[Project page]](https://free4d.github.io/)
[[Paper]](https://arxiv.org/pdf/2503.20785)
[[Code]](https://github.com/TQTQliu/Free4D)
[[Video]](https://youtu.be/GpHnoSczlhA)

Free4D is a tuning-free framework for 4D scene generation from a single image or text.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/dofgaussian.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**CVPR 2025**] [DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting](https://dreamm0ver.github.io/) \\
**Liao Shen**, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy. \\
[[Project page]](https://dof-gaussian.github.io/)
[[Paper]](https://arxiv.org/abs/2503.00746v1)
[[Code]](https://github.com/leoShen917/DoF-Gaussian)

We introduce DoF-Gaussian, a controllable depth-of-field method for 3D-GS. We develop a lens-based imaging model based on geometric optics principles to control DoF effects. Our framework is customizable and supports various interactive applications.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2024</div><img src='images/video_bokeh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ACM MM 2024**] [Video Bokeh Rendering: Make Casual Videography Cinematic](https://dlnext.acm.org/doi/10.1145/3664647.3680629) **<font color=red>(Best paper candidate)</font>**üöÄüöÄüöÄ \\
Yawen Luo, Min Shi, **Liao Shen**, Yachuan Huang, Zixuan Ye, Juewen Peng, Zhiguo Cao.\\
[[Paper]](https://dlnext.acm.org/doi/10.1145/3664647.3680629)

We introduce VBR, the video bokeh rendering model that first leverages information from multiple frames to generate refocusable videos from all-in-focus videos.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/dreammover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**ECCV 2024**] [DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion](https://dreamm0ver.github.io/) \\
**Liao Shen**, Tianqi Liu, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, Zhiguo Cao. \\
[[Project page]](https://dreamm0ver.github.io/)
[[Paper]](https://arxiv.org/abs/2409.09605)
[[Code]](https://github.com/leoShen917/DreamMover)

By leveraging the prior of diffusion models, DreamMover can generate intermediate images from image pairs with large motion while maintaining semantic consistency.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/ft_process.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ECCV 2024**] [MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo](https://mvsgaussian.github.io/) \\
Tianqi Liu, Guangcong Wang, Shoukang Hu, **Liao Shen**, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.  \\
[[Project page]](https://mvsgaussian.github.io/)
[[Paper]](https://arxiv.org/abs/2405.12218)
[[Code]](https://github.com/TQTQliu/MVSGaussian)

MVSGaussian is a Gaussian-based method designed for efficient reconstruction of unseen scenes from sparse views in a single forward pass. It offers high-quality initialization for fast training and real-time rendering.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/dyblurf.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**CVPR 2024**]  [DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video](https://arxiv.org/abs/2403.10103) \\
Huiqiang Sun, Xingyi Li, **Liao Shen**, Xinyi Ye, Ke Xian, Zhiguo Cao.  \\
[[Project page]](https://huiqiang-sun.github.io/dyblurf/)
[[Paper]](https://arxiv.org/abs/2403.10103)
[[Code]](https://github.com/huiqiang-sun/DyBluRF)

DyBluRF is a dynamic neural radiance field method that synthesizes sharp novel views from a monocular video affected by motion blur.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2023</div><img src='images/make-it-4d.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**ACM MM 2023**] [Make-It-4D: Synthesizing a Consistent Long-Term Dynamic Scene Video from a Single Image](https://arxiv.org/abs/2308.10257) \\
**Liao Shen**, Xingyi Li, Huiqiang Sun, Juewen Peng, Ke Xian, Zhiguo Cao, Guosheng Lin. \\
[[Paper]](https://arxiv.org/abs/2308.10257)
[[Code]](https://github.com/leoShen917/Make-It-4D)

Make-It-4D is a novel framework that can generate a consistent long-term dynamic video from a single image. The generated video in volves both visual content movements and large camera motions, bringing the still image back to life.

</div>
</div>
